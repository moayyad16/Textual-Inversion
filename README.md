# Textual-Inversion
## Introduction
For a university graduation project, my team and I started working on generating state-of-the-art images using Textual Inversion technique.
Significant advancements have been made in the field of artificial intelligence, particularly in text-to-image models. These models have shown an impressive ability to generate high-quality, varied images from textual prompts.

However, they still face challenges in accurately mimicking the visual attributes of subjects from specific reference sets and in generating novel interpretations of these subjects in different contexts. In response to these limitations, this project investigates, implements, and uses one of the novel methods for 'personalizing' text-to-image diffusion models, that may enhance their adaptability and accuracy.
Textual Inversion allows individuals to infuse their unique style into generated content. By harnessing the power of Textual Inversion, individuals can create visually captivating and stylistically consistent outputs that reflect their artistic preferences.The concepts learned through Textual Inversion can significantly enhance the control and customization of generated images in text-to-image pipelines. By acquiring new "words" in the embedding space of the text encoder, these concepts can be seamlessly integrated into text prompts, resulting in personalized and highly tailored image generation.
![image](https://github.com/moayyad16/Textual-Inversion/assets/93573570/acf0018e-17a1-4ca5-98e5-f0bbdddc8163)

## The Stable diffusion model
The Stable Diffusion Model (SDM) is an open-source, pre-trained text-to-image latent diffusion model that has been trained on LAION-5B4, consisting of 5 billion image-text pairs. Training such a model takes a very long time and requires significant computing capability. Specifically, the model was trained for 150,000 GPU hours using 256 Nvidia A100 GPUs.
Our textual inversion training is based on the Stable Diffusion Model. Due to the lengthy training time required for the Stable Diffusion Model, we could not afford to train our model from scratch to use as the basis for our textual inversion project.

![304905523-a4c65993-dee3-4d38-8fab-d335f68864f8](https://github.com/moayyad16/Textual-Inversion/assets/93573570/147213ae-d96d-4e3a-bad0-f74d13751671)


## Learning embeddings
The objective of fine-tuning the model in Textual Inversion is to create and refine an embedding that represents a specific concept not originally included in the model's training data. This embedding is associated with a unique token, which is used in text prompts to signify a particular concept. During the training process, the embedding is iteratively updated. This is achieved by comparing the outputs generated by the model when using the special token in prompts against the target outputs (such as images) that exemplify the concept. The update is typically guided by a loss function that measures how well the model's outputs align with the concept as represented in the training dataset. This process adjusts the embedding to embody the concept more accurately, allowing the model to generate content closely aligned with the intended idea or style when the token is used in prompts. The mechanism behind the loss functions is a cosine similarity which is done by CLIP pretrained model. Note that the embeddings updating is limited to the concept embedding, the pre-trained model embeddings are kept untouched.

### CLIP
The similarity is computed between the text and input images on one side, and the generated image on the other, resulting in two values: Clip I and Clip T. This is depicted in the following image
![image](https://github.com/moayyad16/Textual-Inversion/assets/93573570/8938e3e7-8431-4b8c-8785-25b5f03f8c1d)
